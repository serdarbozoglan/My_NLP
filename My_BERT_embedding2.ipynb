{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My_BERT_embedding2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serdarbozoglan/My_NLP/blob/master/My_BERT_embedding2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tadN4hSCP9p",
        "colab_type": "text"
      },
      "source": [
        "# Stage 1: Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUU4TlmoFMZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXj8lk3uGn4P",
        "colab_type": "code",
        "outputId": "7682f548-a805-4a99-ac18-fa77995deac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "  Downloading https://files.pythonhosted.org/packages/c2/d8/14e0cfa03bbeb72c314f0648267c490bcceec5e8fb25081ec31307b5509c/bert-for-tf2-0.12.6.tar.gz\n",
            "Collecting py-params>=0.7.3\n",
            "  Downloading https://files.pythonhosted.org/packages/ec/17/71c5f3c0ab511de96059358bcc5e00891a804cd4049021e5fa80540f201a/py-params-0.8.2.tar.gz\n",
            "Collecting params-flow>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/12/2604f88932f285a473015a5adabf08496d88dad0f9c1228fab1547ccc9b5/params-flow-0.7.4.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (4.28.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.12.6-cp36-none-any.whl size=29115 sha256=f0861b771875d0640973aab9866506052f52ca894ba0c3fa7e2f3e94ff6d6584\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/19/54/51eeca468b219a1bc910c54aff87f0648b28a1fb71c115ba0f\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.8.2-cp36-none-any.whl size=4633 sha256=40db3e5a542914b1a95bd9018dd86be7baa2f432a073d3554e5e4c20a01d2dd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/3a/9c/baf35d6f17f0c2c6b61bf8ac3ab9fc12df0e41432ccaeecacb\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.7.4-cp36-none-any.whl size=16196 sha256=618295274490360c4ebaee692ed91a87f10067c6273af2818d80b73f655a3f2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/30/40/507b60d68b67ac87f35e95c98f5b296a32f146d5ae1d1d5aa7\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.12.6 params-flow-0.7.4 py-params-0.8.2\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOfuPdFHFpfC",
        "colab_type": "code",
        "outputId": "1fa07f08-f103-4058-8cb2-840041d05edc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6ZbE2lPDIFL",
        "colab_type": "text"
      },
      "source": [
        "# Stage 2: Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9S77lewDNE1",
        "colab_type": "text"
      },
      "source": [
        "## Loading files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7GET0xsDSDc",
        "colab_type": "text"
      },
      "source": [
        "We import files from our personal Google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hABc0h8GdTe",
        "colab_type": "code",
        "outputId": "6d7e658b-f317-4915-d3ce-7497df75263a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slnILsqwGxTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "data = pd.read_csv(\n",
        "    \"/content/drive/My Drive/DS_Projects/BERT/sentiment_data/train.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xamH_VIzuNRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Kolaylik olmasi icin sadece ilk 20K ve son 20K yi alacagim data'dan (sirali oludgu icin ilk 20K negative sentiment, last 20K positive sentiment)\n",
        "data1 = data[:20000]\n",
        "data2 = data[-20000:]\n",
        "data = pd.concat([data1, data2], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REdK4z4YG9kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Drop unnecessary columns\n",
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz2g61evDZb4",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCyy4babDrI8",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEyorQS_HArn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    # Removing the @, mentions such as @tigers\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    # Removing the URL links\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    # Keeping only letters\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet) # [^] means \"not\" yani a-zA-Z etc olmayanlari degistir anlaminda\n",
        "    # Removing additional whitespaces\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BlbZpy0HHiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data.text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6SOj46BHKEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_labels = data.sentiment.values\n",
        "\n",
        "# We will convert 4 to 1 because in dataset positive is represented by 4 rather than 1\n",
        "data_labels[data_labels == 4] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJa3YWeJD1gM",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUaCPqqBD7kQ",
        "colab_type": "text"
      },
      "source": [
        "We need to create a BERT layer to have access to meta data for the tokenizer (like vocab size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wry-st-HMN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmAofccpubRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c04d7b6b-9442-4458-a0b0-a1f48906fa83"
      },
      "source": [
        "tokenizer.tokenize(\"My dog loves, strawberries.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my', 'dog', 'loves', ',', 'straw', '##berries', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg_oh5phuevk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5333399c-2ce8-47c5-f1f4-10d8f423d50b"
      },
      "source": [
        "tokenizer.convert_tokens_to_ids(tokenizer.tokenize('My dog loves, strawberries.'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2026, 3899, 7459, 1010, 13137, 20968, 1012]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMVarTJpELyK",
        "colab_type": "text"
      },
      "source": [
        "We only use the first sentence for BERT inputs so we add the CLS token at the beginning and the SEP token at the end of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-JkZt9NduoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_sentence(sent):\n",
        "    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pel_Uk6Ic4xB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z32MeEwnkCB8",
        "colab_type": "text"
      },
      "source": [
        "### Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUVc83VNEcW9",
        "colab_type": "text"
      },
      "source": [
        "We need to create the 3 different inputs for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmW9JZLJaxww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First input\n",
        "def get_ids(tokens): # we get integers for strings\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# Second input\n",
        "# if the tokens==\"[PAD]\" then it will return 1, if they are equalant then we will get 0\n",
        "# Eger token \"[PAD]\" kelimesine esit degilse yani sornam bir stringse 1, \"[PAD]\" kelimesi ise token o zaman 0 dondurur\n",
        "def get_mask(tokens): # padding mask\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "# Third input\n",
        "# Until we see \"[SEP]\" token we will get 0 and after that we will get 1.\n",
        "# Hatirlatma, \"[SEP]\" i ilk cumleden sonra eklemistik yukarida tokenize ederken\n",
        "def get_segments(tokens):\n",
        "    seg_ids =[]\n",
        "    current_seg_id = 0 # for the first sentence we will have 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # When we see token [SEP] we understand that we're in the second sentence # turns 1 into 0 and vice versa\n",
        "            # 2nci cumlenin sonunda tekrar [SEP] i gordugunde 0 olur bu sefer\n",
        "    return seg_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x06fFPFtFqVK",
        "colab_type": "text"
      },
      "source": [
        "We will create padded batches (so we pad sentences for each batch independently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length, apply padded_batches and then shuffle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjAVGCwlb6F8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)]\n",
        "\n",
        "## Initial/original file has ordered labels, first comes 0s then 4s(we converted to 1s later) so we need to shuffle\n",
        "# We shuffle negative and positive sentences\n",
        "random.shuffle(data_with_len)\n",
        "\n",
        "# data_with_len in elemanlari siranyla sentence, label ve sent lenght (row number 42)\n",
        "# we're sorting the list based on the sentence length which is the index of [2] means 3rd element in the list\n",
        "data_with_len.sort(key=lambda x: x[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSHc2i8ru1qF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We sort our necessary inputs\n",
        "# sent_lab is sentence_label\n",
        "sorted_all = [([get_ids(sent_lab[0]),  # which corresponds our sentence\n",
        "                get_mask(sent_lab[0]), # maks\n",
        "                get_segments(sent_lab[0])],  # segments\n",
        "                sent_lab[1]) # label\n",
        "               for sent_lab in data_with_len if sent_lab[2] > 7] # sadece 7 token'dan buyuk olan cumleleri kullanacagiz. Kisa cumleleri disregard edecegiz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkMiqmzsfo6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A list is a type of iterator so it can be used as generator for a dataset\n",
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkGWlzeOfos6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE,\n",
        "                                       padded_shapes=((3, None), ()),\n",
        "                                       padding_values=(0, 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aA7it--hHl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10\n",
        "## all_batched i shuflle etmemiz gerekmektedir yoksa en kisa cumlelerden en uz\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)  # we grap first NUMBER_BATCHES_TEST for validation\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST) # we skip first BUMBER_BATCHES_TEST but rest for training set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv3gih3GvJvt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42f35663-abb0-4d3d-ae9f-162950bb259d"
      },
      "source": [
        "# Ornek bir cumlenin bert layer'a unput olark gonderilmesi asagidadir\n",
        "my_sent = [\"[CLS]\"] + tokenizer.tokenize(\"Roses are red.\") + [\"[SEP]\"]\n",
        "\n",
        "# 3 farkli token'a ihtiyacimiz oldugu yukarida aciklanmisti\n",
        "# Batch i simulate etmek icin t.expand_dims i kullanacagiz, dimension eklemek icin, dimesion i expand ederiz\n",
        "# inputumuzu tensora cevirmek icin tf.cast i kullaniriz, list of tokenlar ise ilgili fonksyonlarla elde edilir ornegin get_ids(my_sent) gibi\n",
        "# dimesionin ilk elemani olarak eklemek icin 0'i kullaniyoruz\n",
        "bert_layer([tf.expand_dims(tf.cast(get_ids(my_sent), tf.int32), 0),\n",
        "            tf.expand_dims(tf.cast(get_mask(my_sent), tf.int32), 0),\n",
        "            tf.expand_dims(tf.cast(get_segments(my_sent), tf.int32), 0)])\n",
        "\n",
        "# Asagidaki ciktida sunu goruruz\n",
        "# Input'un 2 elementi var, ilki shape(1, 768) ki 1 burada simulated batch oldugunu gosteriyor. Ilk element ilk array ile baslayan kisim\n",
        "# 768 ise hidden dimension --> embedding dimension. 768 tane array'deki number'lar bizim TUM CUMLE icin kullaniliyor\n",
        "# Second element'n shape i (1, 6, 768) --> 1, simulated batch. 6 --> number of tokens in our input (CLS + 3 kelime (Roses are red) + nokta (.) + SEP)\n",
        "# 768 --> Embedding dimension of hidden size. Burasi reel emberdder olarak karisimiza cikar ve TUM CUMLENIN embeddingi burasidir\n",
        "# 2 farkli output elde ediyoruz yukarida goruldugu gibi. Modeli ne maksatla kullanacagimiza gore yukaridaki outputlardan birini lullaniriz\n",
        "# Classification icin ilk array, ilk element kullanilir\n",
        "# 2. element token level olarak islem yapacagimiz baska NLP tasklerde kullanilir "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
              " array([[-9.27935660e-01, -4.10335422e-01, -9.65755105e-01,\n",
              "          9.07317996e-01,  8.12914014e-01, -1.74174443e-01,\n",
              "          9.11234617e-01,  3.41952294e-01, -8.74521315e-01,\n",
              "         -9.99989271e-01, -7.78410196e-01,  9.69385266e-01,\n",
              "          9.86160517e-01,  6.36963129e-01,  9.48631346e-01,\n",
              "         -7.51193285e-01, -4.58339691e-01, -7.08104610e-01,\n",
              "          4.62098390e-01, -6.57927275e-01,  7.60414600e-01,\n",
              "          9.99994814e-01, -3.96861047e-01,  3.44166279e-01,\n",
              "          6.16488755e-01,  9.94400024e-01, -7.76633918e-01,\n",
              "          9.38316584e-01,  9.59452271e-01,  7.32879519e-01,\n",
              "         -6.93436861e-01,  2.93080509e-01, -9.93785501e-01,\n",
              "         -1.64551809e-01, -9.67019558e-01, -9.95549560e-01,\n",
              "          5.32935500e-01, -6.88060999e-01,  1.34715745e-02,\n",
              "          2.98195519e-02, -9.18356597e-01,  4.20526326e-01,\n",
              "          9.99988914e-01,  2.52676517e-01,  6.06235743e-01,\n",
              "         -3.50750059e-01, -1.00000000e+00,  4.97585505e-01,\n",
              "         -8.95187497e-01,  9.62561011e-01,  9.43730712e-01,\n",
              "          9.03285623e-01,  1.54699624e-01,  5.86143732e-01,\n",
              "          5.80860496e-01, -4.05053288e-01, -2.76640244e-02,\n",
              "          2.98046023e-01, -2.83075929e-01, -6.47424459e-01,\n",
              "         -6.51523709e-01,  5.43847561e-01, -9.56302106e-01,\n",
              "         -9.22750473e-01,  9.61462975e-01,  8.27475369e-01,\n",
              "         -3.50112736e-01, -4.06405807e-01, -8.74318630e-02,\n",
              "         -9.98741165e-02,  8.96688342e-01,  3.00931633e-01,\n",
              "         -1.51129365e-01, -8.52713704e-01,  8.09592485e-01,\n",
              "          4.00989294e-01, -6.61605895e-01,  1.00000000e+00,\n",
              "         -6.16246402e-01, -9.86407161e-01,  8.90943110e-01,\n",
              "          8.11157584e-01,  5.81394672e-01, -6.33873165e-01,\n",
              "          3.78197819e-01, -1.00000000e+00,  6.76351428e-01,\n",
              "         -2.30612695e-01, -9.92552459e-01,  3.85461330e-01,\n",
              "          6.57650828e-01, -2.90105730e-01,  4.46832627e-01,\n",
              "          6.28524184e-01, -5.58409274e-01, -6.65295124e-01,\n",
              "         -4.72272396e-01, -9.28039372e-01, -3.54472637e-01,\n",
              "         -6.19736195e-01,  1.24534994e-01, -3.48905712e-01,\n",
              "         -4.23184335e-01, -4.20834988e-01,  4.56588596e-01,\n",
              "         -6.14470959e-01, -5.15243411e-01,  5.01909971e-01,\n",
              "          4.29147482e-01,  7.59821951e-01,  4.37516600e-01,\n",
              "         -4.33598429e-01,  6.30962133e-01, -9.59743142e-01,\n",
              "          7.73877501e-01, -3.95737976e-01, -9.87354517e-01,\n",
              "         -6.73180223e-01, -9.92996395e-01,  7.77800322e-01,\n",
              "         -5.05856633e-01, -3.19991142e-01,  9.69388783e-01,\n",
              "         -3.51620585e-01,  3.79092157e-01, -2.21649408e-01,\n",
              "         -9.51505721e-01, -1.00000000e+00, -8.80426884e-01,\n",
              "         -8.34713221e-01, -2.77321458e-01, -4.70461309e-01,\n",
              "         -9.83711958e-01, -9.56730306e-01,  6.61120892e-01,\n",
              "          9.56025898e-01,  1.62189126e-01,  9.99961436e-01,\n",
              "         -5.11206031e-01,  9.59530652e-01, -5.58610082e-01,\n",
              "         -8.00221264e-01,  8.48544002e-01, -5.58320403e-01,\n",
              "          8.33738565e-01,  2.63149589e-01, -7.33846903e-01,\n",
              "          3.16189706e-01, -4.83306617e-01,  6.87450230e-01,\n",
              "         -7.94890046e-01, -3.81298304e-01, -8.71706069e-01,\n",
              "         -9.49488163e-01, -3.62460196e-01,  9.51175690e-01,\n",
              "         -7.62520850e-01, -9.61278081e-01, -1.53293848e-01,\n",
              "         -4.02463704e-01, -5.69815695e-01,  8.52475882e-01,\n",
              "          7.99818158e-01,  5.33586442e-01, -6.96546793e-01,\n",
              "          4.84272391e-01,  2.24440724e-01,  7.31195331e-01,\n",
              "         -8.18208158e-01, -3.58149588e-01,  5.32028556e-01,\n",
              "         -4.41676289e-01, -9.25720930e-01, -9.87607300e-01,\n",
              "         -5.07008016e-01,  5.31485558e-01,  9.93827105e-01,\n",
              "          7.66175330e-01,  4.12393242e-01,  8.83270264e-01,\n",
              "         -3.85666579e-01,  8.81850302e-01, -9.67345238e-01,\n",
              "          9.86407459e-01, -3.13535720e-01,  3.57364029e-01,\n",
              "         -6.57590628e-01,  2.70097345e-01, -8.59113097e-01,\n",
              "          2.32004195e-01,  8.62853050e-01, -9.03662443e-01,\n",
              "         -7.94610143e-01, -2.82699376e-01, -4.76583898e-01,\n",
              "         -5.01097441e-01, -8.80422533e-01,  5.45586228e-01,\n",
              "         -4.41977531e-01, -5.77728987e-01, -1.25810176e-01,\n",
              "          9.06032085e-01,  9.80562210e-01,  8.44253540e-01,\n",
              "          5.20119846e-01,  7.80579567e-01, -9.23414350e-01,\n",
              "         -5.87244034e-01,  2.34754607e-01,  2.97746837e-01,\n",
              "          3.54463905e-01,  9.96218681e-01, -8.01237583e-01,\n",
              "         -2.79998630e-01, -9.39948916e-01, -9.83751893e-01,\n",
              "          3.82993743e-02, -9.28821981e-01, -2.94138104e-01,\n",
              "         -7.00600863e-01,  7.67863989e-01, -3.51922661e-01,\n",
              "          6.57683969e-01,  5.54107249e-01, -9.90459263e-01,\n",
              "         -7.80434132e-01,  5.50272942e-01, -5.03933132e-01,\n",
              "          5.56852698e-01, -3.53224486e-01,  7.86349893e-01,\n",
              "          9.69607174e-01, -6.54101968e-01,  7.34233022e-01,\n",
              "          8.81996691e-01, -9.14772213e-01, -7.82534897e-01,\n",
              "          8.52741420e-01, -4.38667834e-01,  8.24172258e-01,\n",
              "         -7.77354598e-01,  9.91627038e-01,  9.48048651e-01,\n",
              "          7.74401784e-01, -9.52811956e-01, -7.52400279e-01,\n",
              "         -8.65391195e-01, -8.10665190e-01, -1.91085503e-01,\n",
              "          6.22542612e-02,  9.39342856e-01,  6.60155177e-01,\n",
              "          5.10393262e-01,  3.03855479e-01, -7.58786142e-01,\n",
              "          9.97947216e-01, -8.39390755e-01, -9.73821759e-01,\n",
              "         -6.96808457e-01, -4.71226186e-01, -9.92139876e-01,\n",
              "          9.27336991e-01,  3.11118305e-01,  6.17148936e-01,\n",
              "         -5.93245447e-01, -7.30744243e-01, -9.74081218e-01,\n",
              "          9.14184153e-01,  2.35107139e-01,  9.90232766e-01,\n",
              "         -4.98075932e-01, -9.59739566e-01, -7.62371182e-01,\n",
              "         -9.30913448e-01, -4.32067551e-02, -2.13116363e-01,\n",
              "         -6.06085420e-01, -2.81608012e-02, -9.69718456e-01,\n",
              "          6.36244357e-01,  6.35316610e-01,  5.37905693e-01,\n",
              "         -8.91036212e-01,  9.99303162e-01,  1.00000000e+00,\n",
              "          9.73003745e-01,  9.01396990e-01,  8.87466669e-01,\n",
              "         -9.99958754e-01, -6.90022051e-01,  9.99997973e-01,\n",
              "         -9.93730545e-01, -1.00000000e+00, -9.37510908e-01,\n",
              "         -8.12253356e-01,  2.70661116e-01, -1.00000000e+00,\n",
              "         -2.87079096e-01, -1.50920302e-01, -9.31140423e-01,\n",
              "          8.18555057e-01,  9.78329003e-01,  9.94965553e-01,\n",
              "         -1.00000000e+00,  8.81453633e-01,  9.30843234e-01,\n",
              "         -7.06026793e-01,  9.76767182e-01, -6.08330250e-01,\n",
              "          9.75543499e-01,  5.93019485e-01,  5.54319263e-01,\n",
              "         -2.44307384e-01,  4.22843724e-01, -9.68066394e-01,\n",
              "         -9.14158463e-01, -7.75702953e-01, -7.79753625e-01,\n",
              "          9.98873413e-01,  2.67525911e-01, -7.70681083e-01,\n",
              "         -9.30970430e-01,  6.98258400e-01, -1.79435968e-01,\n",
              "          1.48644850e-01, -9.69404459e-01, -3.27199697e-01,\n",
              "          7.69222796e-01,  8.38437557e-01,  2.74363309e-01,\n",
              "          4.46673840e-01, -6.88234031e-01,  4.38525438e-01,\n",
              "         -6.96205944e-02,  2.83885390e-01,  6.96684957e-01,\n",
              "         -9.55272734e-01, -5.49684644e-01, -3.89561653e-01,\n",
              "          3.75222921e-01, -7.64762044e-01, -9.54123020e-01,\n",
              "          9.69721258e-01, -4.86066788e-01,  9.72205758e-01,\n",
              "          1.00000000e+00,  7.64813662e-01, -9.13303673e-01,\n",
              "          6.57082796e-01,  4.31852221e-01, -7.01079607e-01,\n",
              "          1.00000000e+00,  8.67337227e-01, -9.83669639e-01,\n",
              "         -5.84471703e-01,  7.79541016e-01, -6.77890062e-01,\n",
              "         -7.74523914e-01,  9.99660969e-01, -3.41093093e-01,\n",
              "         -8.14479828e-01, -6.48069501e-01,  9.86273110e-01,\n",
              "         -9.94089305e-01,  9.97643113e-01, -8.94537807e-01,\n",
              "         -9.79997218e-01,  9.60477889e-01,  9.49232042e-01,\n",
              "         -6.83828235e-01, -7.17898607e-01,  2.86706746e-01,\n",
              "         -7.60040939e-01,  4.78332579e-01, -9.51963425e-01,\n",
              "          8.08321238e-01,  5.27614355e-01, -1.67665899e-01,\n",
              "          9.16268170e-01, -8.87899101e-01, -5.93430996e-01,\n",
              "          3.90308022e-01, -7.76923358e-01, -3.84818524e-01,\n",
              "          9.59038198e-01,  6.78381324e-01, -4.08702910e-01,\n",
              "         -1.99681204e-02, -4.68428791e-01, -7.41143286e-01,\n",
              "         -9.73734438e-01,  6.23254299e-01,  1.00000000e+00,\n",
              "         -4.31855559e-01,  8.94348860e-01, -5.72569370e-01,\n",
              "         -1.89500619e-02,  7.24835545e-02,  6.05421424e-01,\n",
              "          5.64564288e-01, -5.04035056e-01, -8.33653152e-01,\n",
              "          9.20378685e-01, -9.70664799e-01, -9.92627263e-01,\n",
              "          8.63119602e-01,  2.32818484e-01, -3.05338413e-01,\n",
              "          9.99999225e-01,  6.51024580e-01,  3.69558960e-01,\n",
              "          5.16951859e-01,  9.89937425e-01, -5.10576256e-02,\n",
              "          5.19780993e-01,  9.13519561e-01,  9.89344239e-01,\n",
              "         -4.06514406e-01,  6.72227561e-01,  8.66246223e-01,\n",
              "         -9.63320851e-01, -3.93905610e-01, -7.32534289e-01,\n",
              "          6.66500330e-02, -9.50429142e-01,  5.36765158e-02,\n",
              "         -9.64523733e-01,  9.78591204e-01,  9.72525239e-01,\n",
              "          5.02412975e-01,  3.42612773e-01,  8.20067167e-01,\n",
              "          1.00000000e+00, -8.37067783e-01,  5.97411513e-01,\n",
              "         -4.17202085e-01,  8.81286144e-01, -9.99911010e-01,\n",
              "         -8.37778211e-01, -4.66962188e-01, -2.72496760e-01,\n",
              "         -9.03814495e-01, -4.58637863e-01,  3.91833633e-01,\n",
              "         -9.79059398e-01,  9.10196364e-01,  8.29555571e-01,\n",
              "         -9.92893636e-01, -9.93933320e-01, -5.58821559e-01,\n",
              "          7.86012232e-01,  2.98601002e-01, -9.94314432e-01,\n",
              "         -8.16725373e-01, -6.58431947e-01,  9.07821953e-01,\n",
              "         -4.84596074e-01, -9.59578753e-01, -5.24700880e-01,\n",
              "         -4.26523328e-01,  5.39447546e-01, -3.51429731e-01,\n",
              "          6.03987932e-01,  8.84236634e-01,  6.91960633e-01,\n",
              "         -7.73553789e-01, -3.49987030e-01, -1.82106271e-01,\n",
              "         -8.09592664e-01,  9.06841576e-01, -8.09706271e-01,\n",
              "         -9.76247668e-01, -2.70705611e-01,  1.00000000e+00,\n",
              "         -5.54332733e-01,  8.93760324e-01,  7.55229771e-01,\n",
              "          7.80316353e-01, -1.99225426e-01,  3.35151374e-01,\n",
              "          9.55944121e-01,  3.82269830e-01, -7.57196665e-01,\n",
              "         -9.39320087e-01, -6.35581732e-01, -6.07329130e-01,\n",
              "          7.00571954e-01,  7.23613441e-01,  7.29011118e-01,\n",
              "          8.65883708e-01,  7.64537692e-01,  2.08821103e-01,\n",
              "         -6.98528588e-02, -5.64271992e-04,  9.99799311e-01,\n",
              "         -4.44100142e-01, -1.80671468e-01, -4.89859819e-01,\n",
              "         -2.91431546e-01, -4.25409347e-01, -1.98750034e-01,\n",
              "          1.00000000e+00,  3.56602132e-01,  7.75661588e-01,\n",
              "         -9.93823767e-01, -9.28071022e-01, -9.31738615e-01,\n",
              "          1.00000000e+00,  8.50040436e-01, -7.60715842e-01,\n",
              "          7.18036532e-01,  7.75469124e-01, -1.75162107e-01,\n",
              "          8.09469461e-01, -3.36547822e-01, -3.02385211e-01,\n",
              "          4.57468003e-01,  3.08043808e-01,  9.70232129e-01,\n",
              "         -6.18904173e-01, -9.75721240e-01, -5.94949067e-01,\n",
              "          5.63390970e-01, -9.66651201e-01,  9.99981284e-01,\n",
              "         -6.10340595e-01, -3.60575199e-01, -4.96435910e-01,\n",
              "         -4.91436720e-01,  4.47817475e-01,  2.87388340e-02,\n",
              "         -9.83154774e-01, -3.47387403e-01,  3.09110790e-01,\n",
              "          9.66639042e-01,  3.75864178e-01, -6.41106606e-01,\n",
              "         -8.90264928e-01,  8.92269313e-01,  8.32000077e-01,\n",
              "         -9.59132135e-01, -9.57766473e-01,  9.71166432e-01,\n",
              "         -9.84971166e-01,  7.67819405e-01,  1.00000000e+00,\n",
              "          3.83998960e-01,  4.38051492e-01,  3.52292508e-01,\n",
              "         -4.46136653e-01,  4.46569353e-01, -6.90631747e-01,\n",
              "          6.74425662e-01, -9.59155917e-01, -4.53285158e-01,\n",
              "         -2.96152681e-01,  3.57684523e-01, -2.41154641e-01,\n",
              "         -5.88313937e-01,  7.63308406e-01,  3.13667297e-01,\n",
              "         -6.03100657e-01, -6.84795618e-01, -2.60147393e-01,\n",
              "          5.75160384e-01,  9.16844249e-01, -3.56800258e-01,\n",
              "         -2.31558055e-01,  1.15727797e-01, -1.77119032e-01,\n",
              "         -9.47563648e-01, -5.23142159e-01, -6.04617894e-01,\n",
              "         -9.99998629e-01,  5.41668057e-01, -1.00000000e+00,\n",
              "          6.60002172e-01,  3.39036435e-01, -2.57962286e-01,\n",
              "          8.98434103e-01,  3.58503759e-01,  7.80092061e-01,\n",
              "         -8.63456249e-01, -9.04243767e-01,  2.35174209e-01,\n",
              "          8.47542286e-01, -4.83705103e-01, -7.76437461e-01,\n",
              "         -7.77086973e-01,  4.51547086e-01, -1.20644130e-01,\n",
              "          3.45338106e-01, -7.58304715e-01,  7.38663554e-01,\n",
              "         -2.54878432e-01,  1.00000000e+00,  1.56726778e-01,\n",
              "         -6.47173047e-01, -9.80846524e-01,  3.21544915e-01,\n",
              "         -3.49480152e-01,  1.00000000e+00, -8.88086259e-01,\n",
              "         -9.70758855e-01,  4.17614043e-01, -6.59506440e-01,\n",
              "         -8.39061737e-01,  4.56446081e-01,  7.08391964e-02,\n",
              "         -8.59648943e-01, -9.68725741e-01,  9.56584036e-01,\n",
              "          8.95311177e-01, -6.79162621e-01,  7.91996241e-01,\n",
              "         -3.77204984e-01, -5.99682570e-01,  1.89219356e-01,\n",
              "          9.34770465e-01,  9.87944543e-01,  7.07965136e-01,\n",
              "          9.21087921e-01, -1.59540817e-01, -4.83467728e-01,\n",
              "          9.76640522e-01,  2.95252204e-01,  5.32053530e-01,\n",
              "          3.22658211e-01,  1.00000000e+00,  4.97991800e-01,\n",
              "         -9.31000948e-01, -3.24744791e-01, -9.82841671e-01,\n",
              "         -2.67996281e-01, -9.52166200e-01,  4.53916878e-01,\n",
              "          3.94372553e-01,  9.26190257e-01, -3.09752285e-01,\n",
              "          9.69366491e-01, -9.40263808e-01,  1.66798532e-01,\n",
              "         -8.32233191e-01, -7.04411268e-01,  5.49370646e-01,\n",
              "         -9.30373549e-01, -9.88702416e-01, -9.91572261e-01,\n",
              "          7.38682806e-01, -5.26327193e-01, -9.29532051e-02,\n",
              "          2.77956039e-01,  2.54385650e-01,  5.55893064e-01,\n",
              "          5.70780516e-01, -1.00000000e+00,  9.51999664e-01,\n",
              "          5.82980931e-01,  9.13394094e-01,  9.78624463e-01,\n",
              "          7.49032259e-01,  7.39971995e-01,  3.71186614e-01,\n",
              "         -9.89660680e-01, -9.84848559e-01, -5.31397879e-01,\n",
              "         -3.88979614e-01,  8.49413872e-01,  8.17017198e-01,\n",
              "          8.92696977e-01,  6.16892219e-01, -5.75284600e-01,\n",
              "         -2.86467612e-01, -7.60570705e-01, -7.78939545e-01,\n",
              "         -9.94441748e-01,  5.72188377e-01, -7.72194982e-01,\n",
              "         -9.57696319e-01,  9.67420816e-01, -2.17979059e-01,\n",
              "         -1.75552830e-01, -3.26557130e-01, -9.06777978e-01,\n",
              "          9.35597539e-01,  7.66239583e-01,  1.90596163e-01,\n",
              "          1.53928980e-01,  5.40217638e-01,  9.02483642e-01,\n",
              "          9.40388978e-01,  9.88884807e-01, -9.10143971e-01,\n",
              "          7.88360059e-01, -8.31383109e-01,  6.11195445e-01,\n",
              "          8.21669221e-01, -9.41967785e-01,  3.75133336e-01,\n",
              "          5.49406171e-01, -6.15318477e-01,  3.91501874e-01,\n",
              "         -3.66627276e-01, -9.74752426e-01,  8.88878345e-01,\n",
              "         -3.62838179e-01,  6.53205931e-01, -5.35069942e-01,\n",
              "         -2.21283045e-02, -4.40158248e-01, -3.87567371e-01,\n",
              "         -7.89355040e-01, -6.70902610e-01,  6.87370002e-01,\n",
              "          4.34680313e-01,  9.07510996e-01,  9.13954139e-01,\n",
              "         -1.07544944e-01, -8.54991913e-01, -3.22966009e-01,\n",
              "         -7.80992687e-01, -9.35075521e-01,  9.56621170e-01,\n",
              "         -2.46967390e-01, -1.84675679e-01,  7.18142033e-01,\n",
              "          1.66834295e-01,  9.54272628e-01,  5.20279467e-01,\n",
              "         -5.11346579e-01, -3.58430564e-01, -7.76734591e-01,\n",
              "          9.01379049e-01, -6.45810485e-01, -6.68203354e-01,\n",
              "         -6.79575741e-01,  8.30889463e-01,  4.56940144e-01,\n",
              "          9.99998212e-01, -8.61587405e-01, -9.52708602e-01,\n",
              "         -5.74054956e-01, -4.75623608e-01,  5.11585951e-01,\n",
              "         -7.02607632e-01, -1.00000000e+00,  5.05624413e-01,\n",
              "         -6.52046204e-01,  8.13730836e-01, -8.72139335e-01,\n",
              "          8.09319854e-01, -8.18222165e-01, -9.88196373e-01,\n",
              "         -4.00082618e-01,  3.38945836e-01,  7.67013967e-01,\n",
              "         -5.16352892e-01, -8.75940919e-01,  6.15952253e-01,\n",
              "         -7.52709866e-01,  9.89328504e-01,  8.95710826e-01,\n",
              "         -6.14250720e-01,  2.19650164e-01,  7.52754331e-01,\n",
              "         -8.20389867e-01, -8.05691361e-01,  9.38039422e-01]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 6, 768), dtype=float32, numpy=\n",
              " array([[[-0.07947475,  0.0058074 , -0.31414074, ..., -0.45097268,\n",
              "           0.29333177,  0.23387673],\n",
              "         [ 0.39316052,  0.50336343,  0.24021438, ..., -0.3263564 ,\n",
              "           0.34986094,  0.2067322 ],\n",
              "         [ 0.35789213,  0.10767157, -0.04988849, ..., -0.5082276 ,\n",
              "           0.2504894 , -0.2626875 ],\n",
              "         [-0.2989216 , -0.24708796,  0.07151545, ..., -0.33810058,\n",
              "           0.12699565, -0.09681922],\n",
              "         [-0.36815363, -0.7146525 , -0.2103259 , ...,  0.35395202,\n",
              "           0.33438656, -0.6233475 ],\n",
              "         [ 0.88692284, -0.16996965, -0.29173654, ...,  0.0581652 ,\n",
              "          -0.57759863, -0.320753  ]]], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2pxAPFxGe8r",
        "colab_type": "text"
      },
      "source": [
        "# Stage 3: Model building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6DD3k3qPLDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We're creating a Deep CNN (Convolutional Neural Netowrk) Class which intherits from tf.keras.Model class\n",
        "class DCNNBERTEmbedding(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_filters=50,  # we'll use 50 filters/ feature detectors as default, we will use 50 filters for filter size 2 and 50 filters for 3 and 50 filters for 4 filter size\n",
        "                 FFN_units=512,  # Number of Hidden Units we will use in Dense Layers at the end. We'll have 2 Dense Layers. we'll use FFN_units there\n",
        "                 nb_classes=2,  # We have 2 classes positive and negative\n",
        "                 dropout_rate=0.1,\n",
        "                 name=\"dcnn\"):  # model name we gave\n",
        "\n",
        "        super(DCNNBERTEmbedding, self).__init__(name=name) # We're using super class and initiliaze tf.keras.Model class\n",
        "        \n",
        "\n",
        "        # We're staring to create layers\n",
        "        # We're startting with embedding layer\n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=False) # to use BERT as it is trainable should be False, we're not fine tuning, we freeze the BERT layer by False\n",
        "        \n",
        "        # Srarting Creating CNN layers\n",
        "        # First one will be the size of 2, means it will focus on 2 consecutive words, let's call it bigram\n",
        "        # the width of feature detector is the same as input size, so will have 1 Dimensional vector when we aplly filter, remeber the figure from the lesson. We do not use smaller filter than input size width becasue it is nonnse to split the embedding vector. Whole vector represents a single word\n",
        "        # out stride will be 1\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,     # We shift feature detector to only 1D  # kernel_size= 2 for bigrams\n",
        "                                    padding=\"valid\",   # sometimes feature detectors exceeds the inputs size when it is strided then padding='valid handle this \n",
        "                                    activation=\"relu\") # we only keep the positive results or 0 for the neagive results relu --> max(0,x)\n",
        "        \n",
        "        # We will create the same thing for filter size 3 and 4 as well, they will check the 3 and 4 consecutive words\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        \n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "    \n",
        "        # This layer will get the max pool of feature detector/filter\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "\n",
        "        # We're creating the Feed Forward Neural Network parts (We'll use 2 dense layers)\n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\") # Neuron numbers\n",
        "\n",
        "        # We will apply droputs to avoid overfitting\n",
        "        # Dropout will be applied only in TRAINING not in PREDICTION\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        # Let's create the last Dense layer which is output layer\n",
        "        if nb_classes == 2: # For binary classification\n",
        "            self.last_dense = layers.Dense(units=1, # Don't confused here, if we have binary classes we will have 1 neuron\n",
        "                                           activation=\"sigmoid\")\n",
        "        else: # multi-class\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def embed_with_bert(self, all_tokens):\n",
        "        # Asagidaki \"_\" tum cumleyi temsil eden vektoru temsil ediyor\n",
        "        # embs represents words individually, we will use this one\n",
        "        _, embs = self.bert_layer([all_tokens[:, 0, :], # first element--> all the batches, 0 --> token ids, son eleman (:) means all the values\n",
        "                                   all_tokens[:, 1, :], # first element--> all the batches, 1 --> mask tokens, son eleman (:) means all the values\n",
        "                                   all_tokens[:, 2, :]]) # first element--> all the batches, 2 --> segment tokens (sentence A or B) son eleman (:) means all the values\n",
        "        return embs\n",
        "\n",
        "    def call(self, inputs, training): #training shows if we are in training or not, it is boolean\n",
        "\n",
        "        x = self.embed_with_bert(inputs)\n",
        "\n",
        "        x_1 = self.bigram(x) # batch_size, nb_filters, seq_len-1)\n",
        "        x_1 = self.pool(x_1) # we get the shape of (batch_size, nb_filters)\n",
        "        x_2 = self.trigram(x) # batch_size, nb_filters, seq_len-2)\n",
        "        x_2 = self.pool(x_2) # we get the shape of (batch_size, nb_filters)\n",
        "        x_3 = self.fourgram(x) # batch_size, nb_filters, seq_len-3)\n",
        "        x_3 = self.pool(x_3) # we get the shape of (batch_size, nb_filters)\n",
        "        \n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # we concat the results based on the last parameter of shape, it is nb_filters\n",
        "        # merged shape is (batch_size, 3*nb_filters)\n",
        "\n",
        "        # We're gonna apply our first dense layer\n",
        "        merged = self.dense_1(merged)\n",
        "\n",
        "        # Dropout will be applied if it is in TRAINING\n",
        "        merged = self.dropout(merged, training)\n",
        "\n",
        "        # output layer\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsWpzQz2IQvJ",
        "colab_type": "text"
      },
      "source": [
        "# Stage 4: Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhfUFvWEPOIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HPbZ72KPPnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We're creating our Neural Network\n",
        "Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n",
        "                         FFN_units=FFN_UNITS,\n",
        "                         nb_classes=NB_CLASSES,\n",
        "                         dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpHDseF0QLl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1hdT_JT2Rfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We will save the weight of the trained model to use it later as well\n",
        "checkpoint_path = \"./drive/My Drive/DS_Projects/BERT/ckpt_bert_embedding/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "#max_to_keep shows how many checkpoints will be kept in this folder, we may increase this number if we wanna keep previuos checkpoints as well\n",
        "\n",
        "if ckpt_manager.latest_checkpoint: # if we have a checkpoint in our relevant folder we get True if so not it will return None\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8LHztku2cjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If we want to do anything custom in any epoch or any batch we can do it in the way below\n",
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None): # on each epoch end we will save it and print out \n",
        "        ckpt_manager.save() # we will save the state of model at the end of the each epoch \n",
        "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0C5lNxFTMrA",
        "colab_type": "text"
      },
      "source": [
        "## Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrT8oWZzQNmW",
        "colab_type": "code",
        "outputId": "c5adc41f-7fe3-4fcd-89d7-d4cea963d967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "Dcnn.fit(train_dataset,\n",
        "         epochs=NB_EPOCHS,\n",
        "         callbacks=[MyCustomCallback()])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "   1017/Unknown - 54s 53ms/step - loss: 0.4769 - accuracy: 0.7731Checkpoint saved at ./drive/My Drive/DS_Projects/BERT/ckpt_bert_embedding/.\n",
            "1017/1017 [==============================] - 56s 55ms/step - loss: 0.4769 - accuracy: 0.7731\n",
            "Epoch 2/5\n",
            "1016/1017 [============================>.] - ETA: 0s - loss: 0.4082 - accuracy: 0.8144Checkpoint saved at ./drive/My Drive/DS_Projects/BERT/ckpt_bert_embedding/.\n",
            "1017/1017 [==============================] - 39s 39ms/step - loss: 0.4081 - accuracy: 0.8145\n",
            "Epoch 3/5\n",
            "1016/1017 [============================>.] - ETA: 0s - loss: 0.3555 - accuracy: 0.8417Checkpoint saved at ./drive/My Drive/DS_Projects/BERT/ckpt_bert_embedding/.\n",
            "1017/1017 [==============================] - 39s 38ms/step - loss: 0.3552 - accuracy: 0.8417\n",
            "Epoch 4/5\n",
            "1016/1017 [============================>.] - ETA: 0s - loss: 0.2794 - accuracy: 0.8778Checkpoint saved at ./drive/My Drive/DS_Projects/BERT/ckpt_bert_embedding/.\n",
            "1017/1017 [==============================] - 39s 38ms/step - loss: 0.2791 - accuracy: 0.8778\n",
            "Epoch 5/5\n",
            "1016/1017 [============================>.] - ETA: 0s - loss: 0.2105 - accuracy: 0.9104Checkpoint saved at ./drive/My Drive/DS_Projects/BERT/ckpt_bert_embedding/.\n",
            "1017/1017 [==============================] - 39s 39ms/step - loss: 0.2103 - accuracy: 0.9104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9944657d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8j42Eduxl4c",
        "colab_type": "text"
      },
      "source": [
        "1. The training is almost twice times faster than home-made/custom embedding which is hwon in My_BERT_tokenizer python file\n",
        "\n",
        "2. Gercek dataset/Full dataseti kullanmadigim icin performans degerlendirmesi gercekci olmayabilir ama bunun performansi biraz daha koru olabilir. Evaluation accuracy'lere bakmak lazim\n",
        "\n",
        "3. Daha az overfitting elde ederiz, less biased, less overfit\n",
        "\n",
        "4. Eger test set, twitter datasi olmazsa da BERT embedding daha iyi sonuc verecektir. It generalize better way\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAb_ijA5Idmz",
        "colab_type": "text"
      },
      "source": [
        "# Stage 5: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQN-Y99WIf6m",
        "colab_type": "code",
        "outputId": "428998b1-acd1-4649-fffb-ce52b2d81ca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results = Dcnn.evaluate(test_dataset)\n",
        "print(results)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    113/Unknown - 3s 30ms/step - loss: 0.5088 - accuracy: 0.8053[0.5088285698299915, 0.8053097]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj98dgxnmhak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_prediction(sentence):\n",
        "    tokens = encode_sentence(sentence)\n",
        "\n",
        "    input_ids = get_ids(tokens)\n",
        "    input_mask = get_mask(tokens)\n",
        "    segment_ids = get_segments(tokens)\n",
        "\n",
        "    inputs = tf.stack(\n",
        "        [tf.cast(input_ids, dtype=tf.int32),\n",
        "         tf.cast(input_mask, dtype=tf.int32),\n",
        "         tf.cast(segment_ids, dtype=tf.int32)],\n",
        "         axis=0)\n",
        "    inputs = tf.expand_dims(inputs, 0) # simulates a batch\n",
        "\n",
        "    output = Dcnn(inputs, training=False)\n",
        "\n",
        "    sentiment = math.floor(output*2)\n",
        "\n",
        "    if sentiment == 0:\n",
        "        print(\"Output of the model: {}\\nPredicted sentiment: negative\".format(\n",
        "            output))\n",
        "    elif sentiment == 1:\n",
        "        print(\"Output of the model: {}\\nPredicted sentiment: positive\".format(\n",
        "            output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9jC8UnJgOjS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fa6400b0-a4c5-4da9-c1e8-a2ea99aa8ae8"
      },
      "source": [
        "get_prediction(\"This actor is a deception.\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output of the model: [[0.38071552]]\n",
            "Predicted sentiment: negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aMrBVRbeM29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}