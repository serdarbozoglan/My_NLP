{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My_BERT_tokenizer2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serdarbozoglan/My_NLP/blob/master/My_BERT_tokenizer2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca6rYUxNi_MO",
        "colab_type": "text"
      },
      "source": [
        "# Stage 1: Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76HfPILdC5lD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1h4YVFfDd1t",
        "colab_type": "code",
        "outputId": "13bde4a3-a51f-4082-aca6-ad889496a035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.12.6)\n",
            "Requirement already satisfied: params-flow>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.7.4)\n",
            "Requirement already satisfied: py-params>=0.7.3 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (4.28.1)\n",
            "Requirement already satisfied: numpy in /tensorflow-2.1.0/python3.6 (from params-flow>=0.7.1->bert-for-tf2) (1.17.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMqTwu9jENrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0_xu0I3jFP9",
        "colab_type": "text"
      },
      "source": [
        "# Stage 2: Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v60JTFKojIq5",
        "colab_type": "text"
      },
      "source": [
        "## Loading files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTtO7NkPjKUd",
        "colab_type": "text"
      },
      "source": [
        "We import files from our personal Google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRCxQui8Gqi_",
        "colab_type": "code",
        "outputId": "a5a5e242-0b6a-4cb6-a52a-0394e78d8de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6iT5nxDHLRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "data = pd.read_csv(\n",
        "    '/content/drive/My Drive/DS_Projects/BERT/sentiment_data/train.csv',\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\"\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6RtSwntTh5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Kolaylik olmasi icin sadece ilk 20K ve son 20K yi alacagim data'dan (sirali oludgu icin ilk 20K negative sentiment, last 20K positive sentiment)\n",
        "data1 = data[:20000]\n",
        "data2 = data[-20000:]\n",
        "data = pd.concat([data1, data2], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKnCVewUIBkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Drop unnecessary columns\n",
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba85W232Vgpy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "059d8972-fc26-4d0c-c250-6898c9e8e5f2"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                               text\n",
              "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  is upset that he can't update his Facebook by ...\n",
              "2          0  @Kenichan I dived many times for the ball. Man...\n",
              "3          0    my whole body feels itchy and like its on fire \n",
              "4          0  @nationwideclass no, it's not behaving at all...."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Quzx5tnjUtl",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8hlexmRjXIS",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBSUDL-UP-W_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    # Removing the @, mentions such as @tigers\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    # Removing the URL links\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    # Keeping only letters\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet) # [^] means \"not\" yani a-zA-Z etc olmayanlari degistir anlaminda\n",
        "    # Removing additional whitespaces\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jiMaQsLWiTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data.text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaqLE0fdWtni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_labels = data.sentiment.values\n",
        "\n",
        "# We will convert 4 to 1 because in dataset positive is represented by 4 rather than 1\n",
        "data_labels[data_labels == 4] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eh7sIquja5t",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K59PriX4jgBV",
        "colab_type": "text"
      },
      "source": [
        "We need to create a BERT layer to have access to meta data for the tokenizer (like vocab size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wry-st-HMN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU58a-orV6NR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "321da09e-f2f8-4a19-e387-162b2997744b"
      },
      "source": [
        "tokenizer.tokenize(\"My dog loves, strawberries.\")"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my', 'dog', 'loves', ',', 'straw', '##berries', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JKE7H7nV94F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "efb919bb-63a0-40a3-9097-177e788c75db"
      },
      "source": [
        "tokenizer.convert_tokens_to_ids(tokenizer.tokenize('My dog loves, strawberries.'))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2026, 3899, 7459, 1010, 13137, 20968, 1012]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LggMv7k7Z3Ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_sentence(sent):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGfTo5uIa2is",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-4oGSu5jxUi",
        "colab_type": "text"
      },
      "source": [
        "### Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLg0Z7QOj_YZ",
        "colab_type": "text"
      },
      "source": [
        "We will create padded batches (so we pad sentences for each batch independently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length, apply padded_batches and then shuffle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS_f6gWsLfLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)]\n",
        "\n",
        "## Initial/original file has ordered labels, first comes 0s then 4s(we converted to 1s later) so we need to shuffle\n",
        "random.shuffle(data_with_len)\n",
        "\n",
        "# data_with_len in elemanlari siranyla sentence, label ve sent lenght (row number 42)\n",
        "# we're sorting the list based on the sentence length which is the index of [2] means 3rd element in the list\n",
        "data_with_len.sort(key=lambda x: x[2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzBVPzl7WYFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b94d48f8-febf-40df-9185-38466005638d"
      },
      "source": [
        "# Those ones the last 3 longest sentences in our data set\n",
        "print(data_with_len[-3:])"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1045, 2293, 2017, 1998, 3335, 2017, 1012, 1045, 2064, 1005, 1056, 3524, 2005, 2017, 1012, 2123, 1005, 1056, 2022, 5305, 999, 999, 1998, 1012, 1012, 1012, 1045, 3246, 2115, 7514, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999], 1, 64], [[1045, 2293, 2017, 1998, 3335, 2017, 1012, 1045, 2064, 1005, 1056, 3524, 2005, 2017, 1012, 2123, 1005, 1056, 2022, 5305, 999, 999, 1998, 1012, 1012, 1012, 1045, 3246, 2115, 7514, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999], 1, 66], [[2420, 6229, 2047, 2327, 6718, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029], 1, 116]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOAS8ZW4Wjmc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ff6194f-00a9-4a98-9ec0-117dcc96cc3f"
      },
      "source": [
        "print(data_with_len[:3])\n",
        "# As we can see we have some sentences actually no word (those were cleanded actually) in in but we have label for those"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[], 1, 0], [[], 0, 0], [[], 0, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdcjsWgHWo6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll only keep the sentences with at least 7 tokens and get rid of less\n",
        "sorted_all = [(sent_lab[0], sent_lab[1]) # sent_lab means sentence label\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry0uJJg8lSQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32))\n",
        "# first tf.int32 for inputs [word ids] and second one is for the labels [for 0 and 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzHAhlfTlrcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n",
        "# padded_shapes, (None, ) is used for inputs/token ids and () is used for labels, we will not padd the labels so it shoud be ()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0rF7UEtW8d3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "cceacc18-13c2-46b6-97c4-c53588f2a61f"
      },
      "source": [
        "next(iter(all_batched))\n",
        "# What we get here is, for the first batch we have 32 different inputs with 8 tokens each one and corresponding labels\n",
        "# We don't see any padding here actulaly"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(32, 8), dtype=int32, numpy=\n",
              " array([[ 2196,  2568,  2009,  2134,  1005,  1056,  2147,  4312],\n",
              "        [ 2003,  4469,  8945,  4103, 10094,  2000,  5438, 13255],\n",
              "        [24547, 22345,  2038, 17021,  2545,  1029,   999,  1029],\n",
              "        [ 2038,  2053,  6501,  2005,  1037,  2452,  1997,  5572],\n",
              "        [ 2003,  5305,  1997,  2009,  2182,   999,   999,   999],\n",
              "        [ 2003,  2635,  1037,  3338,  2013,  3752, 16012,   999],\n",
              "        [ 3791,  3637,  2021,  2064,  1005,  1056,  2079,  2009],\n",
              "        [ 4530,  2242,  2011,  2032,  8840,  3363,  3363,  2140],\n",
              "        [ 3398,  2008,  1005,  1055,  1037,  2204,  2028,  2205],\n",
              "        [ 2667,  2000,  2147,  2041,  2129,  2023,  2035,  2573],\n",
              "        [ 2021,  6266,  4665,  3475,  1005,  1056,  2648,  1012],\n",
              "        [ 1998,  2054,  1005,  1055,  2187,  1997,  2256, 16324],\n",
              "        [ 2003,  1037, 10474, 10459,  2099,   999,   999,   999],\n",
              "        [ 7459,  1996,  1046,  2497,  1005,  1055,  2047,  2201],\n",
              "        [ 8038,  2100,  1045,  2288,  2000,  1996,  2502,  8771],\n",
              "        [ 2017,  2180,  1005,  1056,  2113,  2127,  2017,  3046],\n",
              "        [ 2061,  6517,  2009,  1005,  1055, 24057,  2648,  7697],\n",
              "        [ 2079,  2292,  2149,  2113,  2073,  2017,  2203,  2039],\n",
              "        [ 9115, 16787, 15270,  2003,  2200, 26855,  2075,   999],\n",
              "        [10474,  2001,  2428, 18358,  2039,  2197,  2305,  1012],\n",
              "        [ 4299,  1045,  2001,  2045,  2205,  1012,  1012,  1012],\n",
              "        [ 2272,  2006,  3782,  2070,  4620,  2052,  2022,  2204],\n",
              "        [ 3666,  8133,  2386,  2190, 20563,  3468,  3185,  2412],\n",
              "        [15624,  2440,  1997,  4454,  2059,  2017,  7153,  1012],\n",
              "        [ 2183,  2000,  2793,  2204,  2305,   999, 21876, 11057],\n",
              "        [ 2074,  2975,  1996,  5581,  2843,  1997,  2147,   999],\n",
              "        [ 2017,  1005,  2128,  2025,  1037, 10131,  4974,   999],\n",
              "        [ 2009,  1005,  1055,  1996,  5166,  1999,  3163,  1012],\n",
              "        [ 2008,  2003,  2053,  4569,  2000,  6865,  2105,   999],\n",
              "        [ 1045, 10587,  2022,  2019,  2849, 17190,  7174,  1012],\n",
              "        [ 3752,  6933,  2012, 13232, 29564,  2467,  4152,  2033],\n",
              "        [ 2204,  6735,  2012,  3892,  1005,  1055,  2208,  1012]],\n",
              "       dtype=int32)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
              " array([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
              "        1, 1, 1, 0, 0, 0, 0, 1, 0, 1], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrPqJeYpmfcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsM9Kwz9ZGMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "228fd8d6-b748-473b-dfa8-34d2f92cc7a8"
      },
      "source": [
        "print(\"Number of Batches in Test Set :\", NB_BATCHES)\n",
        "print(\"Number of Batches in Test Set :\", NB_BATCHES_TEST) # Actually validation set is this one"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Batches in Test Set : 1032\n",
            "Number of Batches in Test Set : 103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPNQ0GXAZD9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## all_batched i shuflle etmemiz gerekmektedir yoksa en kisa cumlelerden en uzun cumlelere dogru bir siralma var halihaizrda\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)  # we grap first NUMBER_BATCHES_TEST for validation\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST) # we skip first BUMBER_BATCHES_TEST but rest for training set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxONsFVHkFLU",
        "colab_type": "text"
      },
      "source": [
        "# Stage 3: Model building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6DD3k3qPLDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We're creating a Deep CNN (Convolutional Neural Netowrk) Class which intherits from tf.keras.Model class\n",
        "class DCNN(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 emb_dim=128,       # embedding dimension is 128 as default now\n",
        "                 nb_filters=50,     # we'll use 50 filters/ feature detectors as default, we will use 50 filters for filter size 2 and 50 filters for 3 and 50 filters for 4 filter size\n",
        "                 FFN_units=512,     # Number of Hidden Units we will use in Dense Layers at the end. We'll have 2 Dense Layers. we'll use FFN_units there\n",
        "                 nb_classes=2,      # We have 2 classes \n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"dcnn\"):      # model name we gave\n",
        "\n",
        "        super(DCNN, self).__init__(name=name)# We're using super class and initiliaze tf.keras.Model class\n",
        "\n",
        "        # We're staring to create layers\n",
        "        # We're startting with embedding layer\n",
        "        self.embedding = layers.Embedding(vocab_size,\n",
        "                                          emb_dim)  # for vocab_size we create a vector embedding of each word and emd dim is embd_dim here\n",
        "\n",
        "        # Srarting Creating CNN layers\n",
        "        # First one will be the size of 2, means it will focus on 2 consecutive words, let's call it bigram\n",
        "        # the width of feature detector is the same as input size, so will have 1 Dimensional vector when we aplly filter, remeber the figure from the lesson. We do not use smaller filter than input size width becasue it is nonnse to split the embedding vector. Whole vector represents a single word\n",
        "        # out stride will be 1\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,          # We shift feature detector to only 1D  # kernel_size= 2 for bigrams\n",
        "                                    padding=\"valid\",        # sometimes feature detectors exceeds the inputs size when it is strided then padding='valid handle this \n",
        "                                    activation=\"relu\")      # we only keep the positive results or 0 for the neagive results\n",
        "\n",
        "        # We will create the same thing for filter size 3 and 4 as well, they will check the 3 and 4 consecutive words\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        \n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        \n",
        "        # This layer will get the max pool of feature detector/filter\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "\n",
        "        # We're creating the Feed Forward Neural Network parts (We'll use 2 dense layers)\n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\") # Neuron numbers\n",
        "\n",
        "        # We will apply droputs to avoid overfitting\n",
        "        # Dropout will be applied only in TRAINING not in PREDICTION\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        # Let's create the last Dense layer which is output layer\n",
        "        if nb_classes == 2: # For binary classification\n",
        "            self.last_dense = layers.Dense(units=1, # Don't confused here, if we have binary classes we will have 1 neuron\n",
        "                                           activation=\"sigmoid\")\n",
        "        else: # multi-class\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training): #training shows if we are in training or not, it is boolean\n",
        "\n",
        "        # training will decide if we apply droput or not, if it is True it means we're in Trainig  and will aplly droput \n",
        "        # if it is False it measn we're in Prediction and not apply dropout\n",
        "    \n",
        "        x = self.embedding(inputs)\n",
        "        x_1 = self.bigram(x) # batch_size, nb_filters, seq_len-1)\n",
        "        x_1 = self.pool(x_1) # # we get the shape of (batch_size, nb_filters)\n",
        "        x_2 = self.trigram(x) # batch_size, nb_filters, seq_len-2)\n",
        "        x_2 = self.pool(x_2) # # we get the shape of (batch_size, nb_filters)\n",
        "        x_3 = self.fourgram(x) # batch_size, nb_filters, seq_len-3)\n",
        "        x_3 = self.pool(x_3) # # we get the shape of (batch_size, nb_filters)\n",
        "        \n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # we concat the results based on the last parameter of shape, it is nb_filters\n",
        "        # merged shape is (batch_size, 3*nb_filters)\n",
        "\n",
        "        # We're gonna apply our first dense layer\n",
        "        merged = self.dense_1(merged)\n",
        "\n",
        "        # Dropout will be applied if it is in TRAINING\n",
        "        merged = self.dropout(merged, training)\n",
        "\n",
        "        # output layer\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSix1l4jkIxp",
        "colab_type": "text"
      },
      "source": [
        "# Stage 4: Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhfUFvWEPOIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = len(tokenizer.vocab)\n",
        "EMB_DIM = 200           # you can play around with this number as a hyper parameter\n",
        "NB_FILTERS = 100        # you can play around with this number as a hyper parameter\n",
        "FFN_UNITS = 256         # you can play around with this number as a hyper parameter\n",
        "NB_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2      # you can play around with this number as a hyper parameter\n",
        "\n",
        "NB_EPOCHS = 5           # you can play around with this number as a hyper parameter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMtdiWmwv6rD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We're creating our Neural Network\n",
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
        "            emb_dim=EMB_DIM,\n",
        "            nb_filters=NB_FILTERS,\n",
        "            FFN_units=FFN_UNITS,\n",
        "            nb_classes=NB_CLASSES,\n",
        "            dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6apbd7FrwPYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78cceSGCw1XC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We will save the weight of the trained model to use it later as well\n",
        "checkpoint_path = \"./drive/My Drive/DS_Projects/BERT/ckpt_bert_tok/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "#max_to_keep shows how many checkpoints will be kept in this folder, we may increase this number if we wanna keep previuos checkpoints as well\n",
        "\n",
        "if ckpt_manager.latest_checkpoint: # if we have a checkpoint in our relevant folder we get True if so not it will return None\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YIF5trzx7RA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If we want to do anything custom in any epoch or any batch we can do it in the way below\n",
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None): # on each epoch end we will save it and print out \n",
        "        ckpt_manager.save() # we will save the state of model at the end of the each epoch \n",
        "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrT8oWZzQNmW",
        "colab_type": "code",
        "outputId": "cb3d8ae7-0423-4316-fea4-7e7fccb5d5a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "Dcnn.fit(train_dataset,\n",
        "         epochs=NB_EPOCHS,\n",
        "         callbacks=[MyCustomCallback()])"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "    929/Unknown - 60s 65ms/step - loss: 0.5337 - accuracy: 0.7277Checkpoint saved at ./drive/My Drive/DS_Projects/BERT/ckpt_bert_tok/.\n",
            "929/929 [==============================] - 60s 65ms/step - loss: 0.5337 - accuracy: 0.7277\n",
            "Epoch 2/5\n",
            "928/929 [============================>.] - ETA: 0s - loss: 0.3440 - accuracy: 0.8519Checkpoint saved at ./drive/My Drive/DS_Projects/BERT/ckpt_bert_tok/.\n",
            "929/929 [==============================] - 60s 65ms/step - loss: 0.3439 - accuracy: 0.8519\n",
            "Epoch 3/5\n",
            "928/929 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.9471Checkpoint saved at ./drive/My Drive/DS_Projects/BERT/ckpt_bert_tok/.\n",
            "929/929 [==============================] - 59s 64ms/step - loss: 0.1407 - accuracy: 0.9471\n",
            "Epoch 4/5\n",
            "928/929 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9712Checkpoint saved at ./drive/My Drive/DS_Projects/BERT/ckpt_bert_tok/.\n",
            "929/929 [==============================] - 59s 63ms/step - loss: 0.0777 - accuracy: 0.9712\n",
            "Epoch 5/5\n",
            "928/929 [============================>.] - ETA: 0s - loss: 0.0550 - accuracy: 0.9790Checkpoint saved at ./drive/My Drive/DS_Projects/BERT/ckpt_bert_tok/.\n",
            "929/929 [==============================] - 59s 63ms/step - loss: 0.0552 - accuracy: 0.9790\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0d0bfa3748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IiDW919kQQK",
        "colab_type": "text"
      },
      "source": [
        "# Stage 5: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MthhNfnG1TPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "937628d6-0818-4393-cb26-c23ac57c44a0"
      },
      "source": [
        "results = Dcnn.evaluate(test_dataset)\n",
        "print(results)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    103/Unknown - 1s 8ms/step - loss: 0.9869 - accuracy: 0.7418[0.9868806210681073, 0.74180824]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-jrRvtl1xuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_prediction(sentence):\n",
        "    tokens = encode_sentence(sentence)\n",
        "\n",
        "    # We're trying to simulate a batch \n",
        "    # So we add a dimention to tokens and it will be the first one\n",
        "    # We add an empty dimension which simulates the batch \n",
        "    inputs = tf.expand_dims(tokens, 0)\n",
        "\n",
        "    # we don't want to apply dropout so training is False \n",
        "    output = Dcnn(inputs, training=False)\n",
        "\n",
        "    # by multiplying the output by 2 we get a number between 0 and 2\n",
        "    # if it is between 0 and 1 the predicted class will be 0 & if the output is between 1 and 2 the predicition class will be 1\n",
        "    # Instead of this we can also use round() fuction which will yield the same result\n",
        "    sentiment = math.floor(output*2)\n",
        "\n",
        "    \n",
        "    if sentiment == 0:\n",
        "        print(\"Ouput of the model: {}\\nPredicted sentiment: negative.\".format(\n",
        "            output))\n",
        "    elif sentiment == 1:\n",
        "        print(\"Ouput of the model: {}\\nPredicted sentiment: positive.\".format(\n",
        "            output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMyVjgSt3Nze",
        "colab_type": "code",
        "outputId": "d77223d5-92c1-4d4f-9ff9-1528c4f8fcf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "get_prediction(\"This movie was pretty interesting.\")\n",
        "# Training set'i bayagi azalttigimiz icin bu sekilde sonuc almamiz normal"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ouput of the model: [[0.0769823]]\n",
            "Predicted sentiment: negative.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJLuKW7kgq7P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fce36231-d20e-4c82-a4d8-b65c86122e88"
      },
      "source": [
        "get_prediction(\"I like your sweater\")"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ouput of the model: [[0.62157565]]\n",
            "Predicted sentiment: positive.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6HVH7E-gtei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c350fad9-2484-4b2b-c372-d7162062defa"
      },
      "source": [
        "get_prediction(\"I'm so so\")"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ouput of the model: [[0.47023496]]\n",
            "Predicted sentiment: negative.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yzXvXQOgxVZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b17eae4c-4223-41c1-e086-cd61e69b2020"
      },
      "source": [
        "get_prediction(\"I am not bad at all\")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ouput of the model: [[0.0204423]]\n",
            "Predicted sentiment: negative.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daYqMwddg2R_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}